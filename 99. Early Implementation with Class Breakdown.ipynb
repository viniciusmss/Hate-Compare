{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Replication of Hemker (2018)\n",
    "\n",
    "The goal of this notebook is to follow the methodology explained in Hemker (2018) to perform a replication of his results. Note that the source code is not available, rendering this task a bit harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"./data/labeled_data.csv\", index_col=0)\n",
    "raw_tweets = df.tweet\n",
    "raw_labels = df[\"class\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "---\n",
    "\n",
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a raw tweet:\n",
      "\"@Almightywayne__: @JetsAndASwisher @Gook____ bitch fuck u http://t.co/pXmGA68NC1\" maybe you'll get better. Just http://t.co/TPreVwfq0S\n",
      "\n",
      "Its cleaned version is:\n",
      " ||Quotation_Mark|| MENTIONHERE : MENTIONHERE MENTIONHERE bitch fuck u URLHERE ||Quotation_Mark|| maybe you'll get better ||Period|| just URLHERE \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import html\n",
    "\n",
    "def preprocess(text_string):\n",
    "    \n",
    "    # Casing should not make a difference in our case\n",
    "    text_string = text_string.lower()\n",
    "    \n",
    "    # Regex\n",
    "    html_pattern = r'(&(?:\\#(?:(?:[0-9]+)|[Xx](?:[0-9A-Fa-f]+))|(?:[A-Za-z0-9]+));)'    \n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    \n",
    "    # First, add space surrounding HTML entities\n",
    "    text_string = re.sub(html_pattern, r' \\1 ', text_string)\n",
    "    \n",
    "    # Now, if we wish to find hashtags, we have to unescape HTML entities\n",
    "    text_string = html.unescape(text_string)\n",
    "    \n",
    "    # From Udacity TV script generation project\n",
    "    # Replace some punctuation by dedicated tokens\n",
    "    symbol_to_token = {\n",
    "        '.' : '||Period||',\n",
    "        ',' : '||Comma||',\n",
    "        '\"' : '||Quotation_Mark||',\n",
    "        ';' : '||Semicolon||',\n",
    "        '!' : '||Exclamation_Mark||',\n",
    "        '?' : '||Question_Mark||',\n",
    "        '(' : '||Left_Parenthesis||',\n",
    "        ')' : '||Right_Parenthesis||',\n",
    "        '-' : '||Dash||',\n",
    "        '\\n' : '||Return||'\n",
    "    }\n",
    "    \n",
    "    # Next, find URLs\n",
    "    text_string = re.sub(giant_url_regex, ' URLHERE ', text_string)\n",
    "    \n",
    "    # Then, tokenize punctuation\n",
    "    for key, token in symbol_to_token.items():\n",
    "        text_string = text_string.replace(key, ' {} '.format(token))\n",
    "\n",
    "    # Finally, remove spaces and find mentions and hashtags\n",
    "    text_string = re.sub(hashtag_regex, ' HASHTAGHERE ', text_string)\n",
    "    text_string = re.sub(mention_regex, ' MENTIONHERE ', text_string)\n",
    "    text_string = re.sub(space_pattern, ' ', text_string)\n",
    "    \n",
    "    return text_string\n",
    "\n",
    "def _test_preprocess():\n",
    "    \n",
    "    assert \" HASHTAGHERE \" == preprocess(\"#iam1hashtag\")\n",
    "    assert \" URLHERE \" == preprocess(\"https://seminar.minerva.kgi.edu\")\n",
    "    assert \" MENTIONHERE \" == preprocess(\"@vinimiranda\")\n",
    "    assert ' ' == preprocess(\"        \")\n",
    "    assert \" & MENTIONHERE URLHERE HASHTAGHERE \" == \\\n",
    "        preprocess(\"&amp;@vinimiranda    https://seminar.minerva.kgi.edu     #minerva    \")\n",
    "    \n",
    "_test_preprocess()\n",
    "\n",
    "print(\"Example of a raw tweet:\\n{}\".format(raw_tweets[68]))\n",
    "print(\"\\nIts cleaned version is:\\n{}\".format(preprocess(raw_tweets[68])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = raw_tweets.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.329, 'neu': 0.541, 'pos': 0.131, 'compound': -0.6597}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "# Example\n",
    "sentiment_analyzer.polarity_scores(tweets[68])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    24783.000000\n",
       "mean        16.729936\n",
       "std          8.445555\n",
       "min          1.000000\n",
       "25%         10.000000\n",
       "50%         16.000000\n",
       "75%         23.000000\n",
       "max         95.000000\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get cleaned tweets\n",
    "df[\"clean_tweet\"] = tweets\n",
    "\n",
    "# Get their word count\n",
    "df[\"word_count\"] = df.clean_tweet.apply(lambda x : len(x.split()))\n",
    "\n",
    "df.word_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>#Yankees</td>\n",
       "      <td>HASHTAGHERE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24147</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>bitches</td>\n",
       "      <td>bitches</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24218</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>coons</td>\n",
       "      <td>coons</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24869</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>pussy</td>\n",
       "      <td>pussy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count  hate_speech  offensive_language  neither  class     tweet  \\\n",
       "821        3            0                   0        3      2  #Yankees   \n",
       "24147      3            0                   3        0      1   bitches   \n",
       "24218      3            3                   0        0      0     coons   \n",
       "24869      3            0                   3        0      1     pussy   \n",
       "\n",
       "         clean_tweet  word_count  \n",
       "821     HASHTAGHERE            1  \n",
       "24147        bitches           1  \n",
       "24218          coons           1  \n",
       "24869          pussy           1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check tweets with the minimum word count\n",
    "df.loc[df.word_count == df.word_count.min(),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Let's check the tweet(s) with the maximum word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22953</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Was finna slit my eyebrows up in the shop but ...</td>\n",
       "      <td>was finna slit my eyebrows up in the shop but ...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count  hate_speech  offensive_language  neither  class  \\\n",
       "22953      3            0                   0        3      2   \n",
       "\n",
       "                                                   tweet  \\\n",
       "22953  Was finna slit my eyebrows up in the shop but ...   \n",
       "\n",
       "                                             clean_tweet  word_count  \n",
       "22953  was finna slit my eyebrows up in the shop but ...          95  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check tweets with the maximum length\n",
    "df.loc[df.word_count == df.word_count.max(),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's something strange going on. Let's check the tweet again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Was finna slit my eyebrows up in the shop but nahhhhhh.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.word_count == df.word_count.max(),].tweet.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweet contains a lot of new lines. It's hard to know why, but I'll choose to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tweet = df.loc[df.word_count == df.word_count.max(),].tweet.values[0]\n",
    "new_tweet = old_tweet[:old_tweet.find(\"\\r\")]\n",
    "df.loc[df.word_count == df.word_count.max(), \"tweet\"] = new_tweet\n",
    "df.loc[df.word_count == df.word_count.max(), \"clean_tweet\"] = preprocess(new_tweet)\n",
    "df.loc[df.word_count == df.word_count.max(), \"word_count\"] = len(preprocess(new_tweet).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18267</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @TrxllLegend: One good girl is worth a thou...</td>\n",
       "      <td>rt MENTIONHERE : one good girl is worth a thou...</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count  hate_speech  offensive_language  neither  class  \\\n",
       "18267      3            0                   3        0      1   \n",
       "\n",
       "                                                   tweet  \\\n",
       "18267  RT @TrxllLegend: One good girl is worth a thou...   \n",
       "\n",
       "                                             clean_tweet  word_count  \n",
       "18267  rt MENTIONHERE : one good girl is worth a thou...          91  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check tweets with the maximum length\n",
    "df.loc[df.word_count == df.word_count.max(),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rt MENTIONHERE : one good girl is worth a thousand bitches ||Return|| ||Return|| ðŸ‘° = ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ ðŸ‘­ â€¦ '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.word_count == df.word_count.max(),].clean_tweet.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sighes. Well, format-wise it is okay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup Tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: Tweets\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate vocabulary\n",
    "    vocab = set()\n",
    "    text.str.split().apply(vocab.update)\n",
    "    \n",
    "    # Generate lookup tables\n",
    "    vocab_to_int = {word : ii for ii, word in enumerate(vocab, 1)}    \n",
    "    int_to_vocab = {ii : word for word, ii in vocab_to_int.items()}\n",
    "    \n",
    "    # Add padding special word\n",
    "    vocab_to_int['<PAD>'] = 0\n",
    "    int_to_vocab[0] = '<PAD>'\n",
    "    \n",
    "    # return tuple\n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "def _test_lookup_tables():\n",
    "    \n",
    "    text = pd.Series([\"this is a toy\", \"I mean not really a toy\", \"I mean a toy vocabulary\"])\n",
    "    vocab_to_int, int_to_vocab = create_lookup_tables(text)\n",
    "    \n",
    "    # Make sure the dicts make the same lookup\n",
    "    missmatches = [(word, id, id, int_to_vocab[id]) for word, id in vocab_to_int.items() if int_to_vocab[id] != word]\n",
    "    \n",
    "    assert not missmatches,\\\n",
    "        'Found {} missmatche(s). First missmatch: vocab_to_int[{}] = {} and int_to_vocab[{}] = {}'.format(len(missmatches),\n",
    "                                                                                                          *missmatches[0])\n",
    "    \n",
    "_test_lookup_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab = create_lookup_tables(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary is: 21134 tokens.\n",
      "These are 10 randomly sample words in the vocabulary:\n",
      "['fine', 'tht', 'bois', 'caleb', 'tigers', 'plasma', 'moanin', 'ducanville', 'lucricus', 'schoo']\n"
     ]
    }
   ],
   "source": [
    "print(\"The size of the vocabulary is: {} tokens.\".format(len(vocab_to_int)))\n",
    "vocab = list(vocab_to_int.keys())\n",
    "np.random.shuffle(vocab)\n",
    "print(\"These are 10 randomly sample words in the vocabulary:\\n{}\".format(vocab[:10]))\n",
    "del vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Padding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pad_fn(max_length):\n",
    "    \n",
    "    def pad_tweets(tweet, max_length=max_length):\n",
    "        # Do not cut tweet short if it's too long\n",
    "\n",
    "        # Retrieve tweet word count\n",
    "        word_count = len(tweet.split())\n",
    "\n",
    "        # Check how much padding will be needed\n",
    "        n = max_length - word_count if word_count < max_length else 0\n",
    "\n",
    "        # Pad tweet\n",
    "        padded_tweet = ''.join(['<PAD> '] * n + [tweet])\n",
    "\n",
    "        return padded_tweet\n",
    "\n",
    "    return pad_tweets\n",
    "\n",
    "def pad_tweets(tweet, max_length=10):\n",
    "    # Do not cut tweet short if it's too long\n",
    "\n",
    "    # Retrieve tweet word count\n",
    "    word_count = len(tweet.split())\n",
    "    \n",
    "    # Check how much padding will be needed\n",
    "    n = max_length - word_count if word_count < max_length else 0\n",
    "\n",
    "    # Pad tweet\n",
    "    padded_tweet = ''.join(['<PAD> '] * n + [tweet])\n",
    "   \n",
    "    return padded_tweet\n",
    "\n",
    "def _test_pad_tweets():\n",
    "    \n",
    "    assert pad_tweets('hi', 0) == 'hi'\n",
    "    assert pad_tweets('hi', 1) == 'hi'\n",
    "    assert pad_tweets('hi', 2) == '<PAD> hi'\n",
    "    assert len(pad_tweets('hi', 10).split()) == 10\n",
    "    assert len(pad_tweets('hi', 100).split()) == 100\n",
    "    assert pad_tweets('this sentence is a bit longer', 1) == 'this sentence is a bit longer'\n",
    "    \n",
    "_test_pad_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>  ||Quotation_Mark|| keeks is a bitch she curves everyone ||Quotation_Mark|| lol i walked into a conversation like this ||Period|| smh\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = df.word_count.max()\n",
    "pad_tweets = create_pad_fn(MAX_LENGTH)\n",
    "df[\"padded_tweets\"] = df.clean_tweet.map(pad_tweets)\n",
    "print(df.padded_tweets[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "  4186  6191 19261 19729 15454  2584 16026 20818  4186 14489 14937 13904\n",
      "  4989 19729  8343 11852 17110 10422  3627]\n"
     ]
    }
   ],
   "source": [
    "tweets_ints = np.array([[vocab_to_int[word] for word in tweet.split()] for tweet in df.padded_tweets.values])\n",
    "print(tweets_ints[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hate Subclass Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "\n",
    "def hate_classification(hate_tweet):\n",
    "    '''Receives a hateful tweet. \n",
    "       Return 3 for directed hate speech and 4 otherwise.'''\n",
    "    \n",
    "    if bool(hate_tweet.count(\"MENTIONHERE\")): return(3)\n",
    "    \n",
    "    # Remove tokens since they will oncused the POS tagger\n",
    "    token_regex = '\\|\\|\\w+\\|\\|'\n",
    "    hate_tweet = re.sub(token_regex, \"\", hate_tweet)\n",
    "    \n",
    "    # URLHERE is considered a proper noun by the pos tagger.\n",
    "    # Remove them before checking for proper nouns\n",
    "    no_punct_hate = ''.join([char for char in hate_tweet if char not in punctuation])\n",
    "    no_URL_hate = ' '.join([token for token in no_punct_hate.split() if token != \"URLHERE\"])\n",
    "    has_NE = False\n",
    "    for sent in sent_tokenize(no_URL_hate):\n",
    "        for chunk in ne_chunk(pos_tag(word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label'):\n",
    "                return(3)  # Named Entity found    \n",
    "\n",
    "    return(4)\n",
    "        \n",
    "def _test_hate_classification():\n",
    "    assert hate_classification(\"MENTIONHERE\") == 3\n",
    "    assert hate_classification(\"Karen is absolutely crazy\") == 3\n",
    "    assert hate_classification(\"Karen is his sister. She's absolutely crazy\") == 3\n",
    "    assert hate_classification(\"They should all be sent to Mexico\") == 3\n",
    "    assert hate_classification(\"They should all leave the country\") == 4\n",
    "    assert hate_classification(\"some hate speech stuff\") == 4\n",
    "    assert hate_classification(\"\") == 4\n",
    "\n",
    "_test_hate_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a hateful tweet: \n",
      " ||Quotation_Mark|| we're out here ||Comma|| and we're queer ||Exclamation_Mark|| ||Quotation_Mark|| ||Return|| ||Quotation_Mark|| 2 ||Comma|| 4 ||Comma|| 6 ||Comma|| hut ||Exclamation_Mark|| we like it in our butt ||Exclamation_Mark|| ||Quotation_Mark|| \n",
      "Its type of hate speech is: Generalized\n",
      "\n",
      "Example of a hateful tweet:\n",
      " ||Quotation_Mark|| MENTIONHERE : jackies a retard HASHTAGHERE ||Quotation_Mark|| at least i can make a grilled cheese ||Exclamation_Mark|| \n",
      "Its type of hate speech is: Directed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hate_tweets = tweets[df[\"class\"] == 0].values\n",
    "_hate_prnt = lambda x : \"Generalized\" if hate_classification(x) == 4 else \"Directed\"\n",
    "\n",
    "print(\"Example of a hateful tweet: \\n{}\".format(hate_tweets[20]))\n",
    "print(\"Its type of hate speech is: {}\\n\".format(_hate_prnt(hate_tweets[20])))\n",
    "\n",
    "print(\"Example of a hateful tweet:\\n{}\".format(hate_tweets[10]))\n",
    "print(\"Its type of hate speech is: {}\\n\".format(_hate_prnt(hate_tweets[10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change hate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_hate_labels(tweets, raw_labels):\n",
    "    ''' Change hate speech labels (0) to directed (3) / generalized labels (4) \n",
    "        Shifts class numbers to the left so that class labels start from zero.\n",
    "        Returned labels:\n",
    "        \n",
    "            (0) : Offensive\n",
    "            (1) : Neither\n",
    "            (2) : Directed hate speech\n",
    "            (3) : Generalized hate speech\n",
    "    \n",
    "    '''\n",
    "    labels = raw_labels.copy()\n",
    "\n",
    "    for i, (tweet, label) in enumerate(zip(tweets, raw_labels)):\n",
    "\n",
    "        if label == 0:  # If hate speech\n",
    "            labels[i] = hate_classification(tweet)\n",
    "            \n",
    "    return labels - 1 \n",
    "\n",
    "def _test_hate_labels(tweets, raw_labels):\n",
    "    labels = change_hate_labels(tweets, raw_labels)\n",
    "    \n",
    "    assert 4 not in pd.Series(labels).value_counts().index\n",
    "    assert 2 in pd.Series(labels).value_counts().index\n",
    "    assert 3 in pd.Series(labels).value_counts().index\n",
    "    \n",
    "_test_hate_labels(tweets, raw_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    19190\n",
       "1     4163\n",
       "2      954\n",
       "3      476\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the counts for each class\n",
    "labels = change_hate_labels(tweets, raw_labels)\n",
    "pd.Series(labels).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "---\n",
    "### Check Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Training, Validation, and Test Sets\n",
    "\n",
    "all from Udacity script generator project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(19826, 91) \n",
      "Validation set: \t(2478, 91) \n",
      "Test set: \t\t(2479, 91)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "tweets_ints, labels = shuffle(tweets_ints, labels)\n",
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "split_idx = int(tweets_ints.shape[0]*split_frac)\n",
    "train_x, remaining_x = tweets_ints[:split_idx], tweets_ints[split_idx:]\n",
    "train_y, remaining_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 64\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([64, 91])\n",
      "Sample input: \n",
      " tensor([[    0,     0,     0,  ..., 13463, 13936,  1626],\n",
      "        [    0,     0,     0,  ...,  3130, 17484, 15454],\n",
      "        [    0,     0,     0,  ..., 11999,  5941,  2691],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ..., 15671, 18156,  6511],\n",
      "        [    0,     0,     0,  ..., 18397, 18397, 18397],\n",
      "        [    0,     0,     0,  ..., 10690,   742, 15454]], dtype=torch.int32)\n",
      "\n",
      "Sample label size:  torch.Size([64])\n",
      "Sample label: \n",
      " tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HateSpeechClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, cnn_params, pool_params,\n",
    "                 hidden_dim, n_layers, dropout=0.5, pretrained_embed=False, vocab_to_int=None):\n",
    "        \"\"\"\n",
    "        TO BE RESTATED\n",
    "        Initialize the PyTorch RNN Module\n",
    "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
    "        :param output_size: The number of output dimensions of the neural network\n",
    "        :param embedding_dim: The size of embeddings, should you choose to use them\n",
    "        :param cnn_params: A 4-element tuple containing the number \n",
    "            of feature maps, kernel size, stride and padding of a Conv1D layer. \n",
    "        :param pool_params: A 3-element tuple containing the kernel size, stride and padding of a MaxPool1D layer. \n",
    "        :param hidden_dim: The size of the hidden layer outputs\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        super(HateSpeechClassifier, self).__init__()\n",
    "       \n",
    "        # set class variables\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_to_int = vocab_to_int\n",
    "        \n",
    "        # define model layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if pretrained_embed:\n",
    "            self.set_pretrained_weights()\n",
    "            \n",
    "        self.conv = nn.Conv1d(embedding_dim, *cnn_params)\n",
    "        \n",
    "        self.pool = nn.MaxPool1d(*pool_params)\n",
    "        \n",
    "        n_maps, _, _, _ = cnn_params\n",
    "        self.lstm = nn.LSTM(n_maps, hidden_dim, n_layers, \n",
    "                            dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, nn_input, hidden, test_print=False):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param nn_input: The input to the neural network\n",
    "        :param hidden: The hidden state        \n",
    "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
    "        \"\"\"\n",
    "        # TODO: Implement function   \n",
    "        batch_size = nn_input.size(0)\n",
    "\n",
    "        # embeddings\n",
    "        nn_input = nn_input.long()\n",
    "        embeds = self.embedding(nn_input)\n",
    "        \n",
    "        # Change axes. embedding_dim (in_channels) should be in the middle\n",
    "        # [batch_size, seq_length, embedding_dim] -> [batch_size, embedding_dim, seq_length]\n",
    "        embeds_t = embeds.permute(0, 2, 1)\n",
    "        \n",
    "        # conv\n",
    "        conv_out = self.conv(embeds_t)\n",
    "        \n",
    "        # pool\n",
    "        pool_out = self.pool(F.relu(conv_out))\n",
    "        \n",
    "        # Change axes. lstm expects features to be the last channel\n",
    "        # [batch_size, n_maps, down_sampled_seq] -> [batch_size, down_sampled_seq, n_maps]\n",
    "        pool_out_t = pool_out.permute(0, 2, 1)\n",
    "        \n",
    "        # lstm\n",
    "        lstm_out, hidden = self.lstm(pool_out_t, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        # out = self.dropout(lstm_out)\n",
    "        fc_out = self.fc(lstm_out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        fc_out_t = fc_out.view(batch_size, -1, self.output_size)  \n",
    "        \n",
    "        out = fc_out_t[:, -1] # get last batch of labels\n",
    "        \n",
    "        if test_print:\n",
    "            print(\"nn_input.\\nexpected : [batch_size, seq_length].\\nshape: {}\\n\".format(nn_input.shape))\n",
    "            print(\"embeds.\\nexpected : [batch_size, seq_length, embedding_dim].\\nshape: {}\\n\".format(embeds.shape))\n",
    "            print(\"embeds_t.\\nexpected : [batch_size, embedding_dim, seq_length].\\nshape: {}\\n\".format(embeds_t.shape))\n",
    "            print(\"conv_out.\\nexpected : [batch_size, n_maps, seq_length].\\nshape: {}\\n\".format(conv_out.shape))\n",
    "            print(\"pool_out.\\nexpected : [batch_size, n_maps, down_sampled_seq].\\nshape: {}\\n\".format(pool_out.shape))\n",
    "            print(\"pool_out_t.\\nexpected : [batch_size, down_sampled_seq, n_maps].\\nshape: {}\\n\".format(pool_out_t.shape))\n",
    "            print(\"lstm_out.\\nexpected : [batch_size, down_sampled_seq, hidden_dim].\\nshape: {}\\n\".format(lstm_out.shape))\n",
    "            print(\"lstm_out.\\nexpected : [batch_size * down_sampled_seq, hidden_dim].\\nshape: {}\\n\".format(lstm_out.shape))\n",
    "            print(\"fc_out.\\nexpected : [batch_size * down_sampled_seq, output_dim].\\nshape: {}\\n\".format(fc_out.shape))\n",
    "            print(\"fc_out_t.\\nexpected : [batch_size, down_sampled_seq, output_dim].\\nshape: {}\\n\".format(fc_out_t.shape))\n",
    "            print(\"out.\\nexpected : [batch_size, output_dim].\\nshape: {}\\n\".format(out.shape))\n",
    "                  \n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    def set_pretrained_weights(self, model_path=\"glove/glove.twitter.27B.200d.txt\", pnt=True):\n",
    "        \n",
    "        if not hasattr(self, 'word2vec_model'):\n",
    "            self.word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(model_path)\n",
    "\n",
    "        # Check whether the pretrained model has the correct dimensionality\n",
    "        assert len(self.word2vec_model[\"human\"]) == self.embedding_dim\n",
    "        \n",
    "        # Create the lookup table\n",
    "        embedding_weights = np.zeros((len(self.vocab_to_int), self.embedding_dim))\n",
    "\n",
    "        n = 0  # For each word in the dictionary\n",
    "        for word, value in vocab_to_int.items():\n",
    "\n",
    "            try:\n",
    "                # Find its embeddings\n",
    "                embedding_weights[value] = self.word2vec_model[word]\n",
    "\n",
    "            except:\n",
    "                # Or report that it's missing\n",
    "                n += 1\n",
    "            \n",
    "        if pnt: print(\"{} words in the vocabulary have no pre-trained embedding.\".format(n))\n",
    "\n",
    "        device = \"cuda:0\" if train_on_gpu else \"cpu\"\n",
    "        embedding_weights = torch.Tensor(embedding_weights).type(torch.FloatTensor).to(device)\n",
    "        self.embedding.weight = nn.Parameter(embedding_weights)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM/GRU\n",
    "        :param batch_size: The batch_size of the hidden state\n",
    "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
    "        '''\n",
    "        # Implement function\n",
    "        \n",
    "        # initialize hidden state with zero weights, and move to GPU if available\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21132 words in the vocabulary have no pre-trained embedding.\n"
     ]
    }
   ],
   "source": [
    "def _test_HateSpeechClassifier():\n",
    "    batch_size = 20\n",
    "    sequence_length = 14\n",
    "    vocab_size = 3\n",
    "    output_size= 4\n",
    "    embedding_dim= 200\n",
    "    hidden_dim = 12\n",
    "    n_layers = 2\n",
    "    cnn_params = (5, 3, 1, 1)\n",
    "    pool_params = (2, 2, 0)\n",
    "    vocab_to_int = {'banana' : 0, 'apple' : 1, 'orange' : 2}\n",
    "    \n",
    "    # Initialize model\n",
    "    test_classifier = HateSpeechClassifier(vocab_size, output_size, embedding_dim, \n",
    "                                           cnn_params, pool_params, hidden_dim, n_layers,\n",
    "                                           pretrained_embed=True, vocab_to_int=vocab_to_int)\n",
    "    \n",
    "    # create test input\n",
    "    X_npy = np.random.randint(vocab_size, size=(batch_size, sequence_length))\n",
    "    X = torch.from_numpy(X_npy)\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    if(train_on_gpu):\n",
    "        test_classifier.cuda()\n",
    "        X = X.cuda()\n",
    "    \n",
    "    # Compute\n",
    "    hidden = test_classifier.init_hidden(batch_size)\n",
    "    out, hidden_out = test_classifier(X, hidden)\n",
    "    \n",
    "    # Test output and hidden state shapes\n",
    "    assert out.shape == (batch_size, output_size)\n",
    "    assert hidden_out[0].size() == (n_layers, batch_size, hidden_dim)\n",
    "    assert len(test_classifier.embedding.weight.data.shape) == 2\n",
    "    assert test_classifier.embedding.weight.data.shape[0] == len(vocab_to_int)\n",
    "\n",
    "    \n",
    "_test_HateSpeechClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Forward Pass and Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_back_prop(model, optimizer, criterion, inp, target, hidden, clip=5):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    :param model: The PyTorch Module that holds the neural network\n",
    "    :param optimizer: The PyTorch optimizer for the neural network\n",
    "    :param criterion: The PyTorch loss function\n",
    "    :param inp: A batch of input to the neural network\n",
    "    :param target: The target output for the batch of input\n",
    "    :return: The loss and the latest hidden state Tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = inp.size(0)\n",
    "    target = target.type(torch.LongTensor)\n",
    "    \n",
    "    # move data to GPU, if available\n",
    "    if train_on_gpu:\n",
    "        inp, target = inp.cuda(), target.cuda()\n",
    "    \n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    hidden = tuple([each.data for each in hidden])\n",
    "    \n",
    "    # zero accumulated gradients\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # get the output from the model\n",
    "    output, hidden = model(inp, hidden)\n",
    "    \n",
    "    # perform backpropagation and optimization\n",
    "    # calculate the loss and perform backprop\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    try:\n",
    "        loss.backward()\n",
    "    \n",
    "    except RuntimeError:\n",
    "        fn = lambda x, y : print('{} : {}'.format(x, y.shape))\n",
    "        fn('output', output)\n",
    "        fn('target', target)\n",
    "        fn('loss', loss.item())\n",
    "    \n",
    "    # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to understand this code better but I mean it works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "class _TestNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(_TestNN, self).__init__()\n",
    "        self.decoder = torch.nn.Linear(input_size, output_size)\n",
    "        self.forward_called = False\n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        self.forward_called = True\n",
    "        output = self.decoder(nn_input)\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "def _test_forward_back_prop(classifierNN, forward_back_prop, train_on_gpu):\n",
    "    batch_size = 20\n",
    "    sequence_length = 14\n",
    "    input_size = 20\n",
    "    output_size= 4\n",
    "    embedding_dim= 16\n",
    "    hidden_dim = 12\n",
    "    n_layers = 2\n",
    "    cnn_params = (5, 3, 1, 1)\n",
    "    pool_params = (2, 2, 0)\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    model = classifierNN(input_size, output_size, embedding_dim, \n",
    "                         cnn_params, pool_params, hidden_dim, n_layers)\n",
    "    \n",
    "    mock_decoder = MagicMock(wraps=_TestNN(input_size, output_size))\n",
    "    if train_on_gpu:\n",
    "        mock_decoder.cuda()\n",
    "    \n",
    "    mock_decoder_optimizer = MagicMock(wraps=torch.optim.Adam(mock_decoder.parameters(), lr=learning_rate))\n",
    "    mock_criterion = MagicMock(wraps=torch.nn.CrossEntropyLoss())\n",
    "    \n",
    "    with patch.object(torch.autograd, 'backward', wraps=torch.autograd.backward) as mock_autograd_backward:\n",
    "        inp = torch.FloatTensor(np.random.rand(batch_size, input_size))\n",
    "        target = torch.LongTensor(np.random.randint(output_size, size=batch_size))\n",
    "        \n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        \n",
    "        loss, hidden_out = forward_back_prop(mock_decoder, mock_decoder_optimizer, mock_criterion, inp, target, hidden)\n",
    "        \n",
    "    assert (hidden_out[0][0]==hidden[0][0]).sum()==batch_size*hidden_dim\n",
    "    assert mock_decoder.zero_grad.called or mock_decoder_optimizer.zero_grad.called, 'Didn\\'t set the gradients to 0.'\n",
    "    assert mock_decoder.forward_called, 'Forward propagation not called.'\n",
    "    assert mock_autograd_backward.called, 'Backward propagation not called'\n",
    "    assert mock_decoder_optimizer.step.called, 'Optimization step not performed'\n",
    "    assert type(loss) == float, 'Wrong return type. Expected {}, got {}'.format(float, type(loss))\n",
    "    \n",
    "_test_forward_back_prop(HateSpeechClassifier, forward_back_prop, train_on_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, batch_size, optimizer, criterion, n_epochs, train_loader, valid_loader,\n",
    "                     show_every_n_batches=10, try_load = False, save_path=\"model.pt\"):\n",
    "    \n",
    "    # Load model previously trained if availabale\n",
    "    if try_load:\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(save_path))\n",
    "            return model\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # n steps\n",
    "    steps = 0\n",
    "    \n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf  \n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        # initialize variables to monitor training loss\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        \n",
    "        # initialize hidden state\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        \n",
    "        # Set model for training\n",
    "        model.train()\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            # make sure you iterate over completely full batches, only\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            # forward, back prop\n",
    "            loss, hidden = forward_back_prop(model, optimizer, criterion, inputs, labels, hidden)          \n",
    "            \n",
    "            # record loss\n",
    "            train_loss += loss\n",
    "\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print(\"Epoch: {}/{}. \\tBatch: {}/{}.\\t Avg. Training Loss: {}\".format(epoch_i,\n",
    "                                                                                      n_epochs,\n",
    "                                                                                      batch_i, \n",
    "                                                                                      len(train_loader), \n",
    "                                                                                      train_loss/batch_i))\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        \n",
    "        valid_loss = 0.0\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        valid_hidden = model.init_hidden(batch_size)\n",
    "        \n",
    "        # Set model for evaluation\n",
    "        model.eval()\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(valid_loader, 1):\n",
    "            \n",
    "            # make sure you iterate over completely full batches, only\n",
    "            n_batches = len(valid_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            labels = labels.type(torch.LongTensor)\n",
    "\n",
    "            # move data to GPU, if available\n",
    "            if train_on_gpu:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state\n",
    "            valid_hidden = tuple([each.data for each in valid_hidden])\n",
    "\n",
    "            # get the output from the model\n",
    "            output, valid_hidden = model(inputs, valid_hidden)\n",
    "\n",
    "            # calculate the loss \n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            # update running validation loss \n",
    "            valid_loss += loss\n",
    "            \n",
    "            # convert output probabilities to predicted class\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            \n",
    "            # compare predictions to true label\n",
    "            correct += np.sum(np.squeeze(pred.eq(labels.data.view_as(pred))).cpu().numpy())\n",
    "            total += inputs.size(0)\n",
    "\n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = train_loss/len(train_loader)\n",
    "        valid_loss = valid_loss/len(valid_loader)\n",
    "        acc = 100. * correct / total\n",
    "        \n",
    "        # print validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\t Accuracy: {:.6f}\\n'.format(\n",
    "            epoch_i, \n",
    "            train_loss,\n",
    "            valid_loss,\n",
    "            acc\n",
    "            ))\n",
    "        \n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...\\n'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "                \n",
    "    # returns a trained classifier\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = tweets_ints.shape[1]  # number of words in a sequence\n",
    "num_epochs = 5\n",
    "learning_rate = 0.0005\n",
    "vocab_size = len(vocab_to_int)\n",
    "output_size = pd.Series(labels).nunique()\n",
    "embedding_dim = 200\n",
    "hidden_dim = 256\n",
    "batch_size = 64\n",
    "n_layers = 2\n",
    "show_every_n_batches = 50\n",
    "cnn_params = (32, 25, 1, 4)\n",
    "pool_params = (4, 4, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the Model and Train the Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HateSpeechClassifier(vocab_size, output_size, embedding_dim, cnn_params, pool_params,\n",
    "                             hidden_dim, n_layers, dropout=0.5, pretrained_embed=1, vocab_to_int=vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to gpu if available    \n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_pretrained_weights()\n",
    "model = train_classifier(model, batch_size, optimizer, criterion, num_epochs, train_loader, valid_loader,\n",
    "                         show_every_n_batches=show_every_n_batches, save_path=\"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_loss = 0 # track loss\n",
    "num_correct = 0\n",
    "total = 0\n",
    "y_pred, y_true = [], []\n",
    "\n",
    "# init hidden state\n",
    "test_hidden = model.init_hidden(batch_size)\n",
    "\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for batch_i, (inputs, labels) in enumerate(test_loader, 1):\n",
    "\n",
    "    # make sure you iterate over completely full batches, only\n",
    "    n_batches = len(test_loader.dataset)//batch_size\n",
    "    if(batch_i > n_batches):\n",
    "        break\n",
    "                \n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    test_hidden = tuple([each.data for each in test_hidden])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output, test_hidden = model(inputs, test_hidden)\n",
    "    \n",
    "    # Accumulate loss\n",
    "    test_loss += criterion(output, labels)\n",
    "    \n",
    "    # convert output probabilities to predicted class\n",
    "    pred = output.data.max(1, keepdim=True)[1]\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    num_correct += np.sum(np.squeeze(pred.eq(labels.data.view_as(pred))).cpu().numpy())\n",
    "    total += inputs.size(0)\n",
    "    \n",
    "    # Save prediction and labels\n",
    "    y_pred += list(pred.squeeze().cpu().numpy())\n",
    "    y_true += list(labels.data.cpu().numpy())\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(test_loss/len(test_loader)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.1f}%\".format(100*test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "class_names = np.array([\"Offensive\",\"Neither\",\"Dir. Hate\",\"Gen. Hate\"])\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          title=None, normalize=False,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        \n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    \n",
    "    bottom, top = ax.get_ylim()\n",
    "    ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_true, y_pred, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_true, y_pred, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:capstone]",
   "language": "python",
   "name": "conda-env-capstone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
